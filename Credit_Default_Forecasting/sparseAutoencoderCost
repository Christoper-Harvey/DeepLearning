function [cost,grad] = sparseAutoencoderCost(theta, visibleSize, hiddenSize, ...
                                             lambda, sparsityParam, beta, data)
 
W1 = reshape(theta(1:hiddenSize*visibleSize), hiddenSize, visibleSize);
W2 = reshape(theta(hiddenSize*visibleSize+1:2*hiddenSize*visibleSize), visibleSize, hiddenSize);
b1 = theta(2*hiddenSize*visibleSize+1:2*hiddenSize*visibleSize+hiddenSize);
b2 = theta(2*hiddenSize*visibleSize+hiddenSize+1:end);


cost = 0;
W1grad = zeros(size(W1)); 
W2grad = zeros(size(W2));
b1grad = zeros(size(b1));
b2grad = zeros(size(b2)); 

m = columns(data);
a1 = data; %dimen: 64 x examples
z1 = (W1 * a1) + repmat(b1, 1, m); %dimen: # of hidden units x # of examples (ex: 2 x 10)
a2 = sigmoid(z1);
z2 = (W2 * a2) + repmat(b2, 1, m);
a3 = sigmoid(z2); %dimen: # of output units x # of examples (ex: 64 x 10)

cost = sum(sum((data - a3).^2)) / (2 * m);
cost = cost + (lambda / 2) * (sum(sum(W1.^2)) + sum(sum(W2.^2)));
avg_activation = mean(a2,2); %dimen: hidden_units x 1 (ex: 2 x 1)

cost = cost + beta * sum(sparsityParam * log(sparsityParam ./ avg_activation) + (1 - sparsityParam) * log((1 - sparsityParam) ./ (1 - avg_activation)));

%backprop:
delta3 = -(data - a3) .* sigmoid_gradient(z2); %dimen: 64 x 10
sparsity_delta = repmat(beta * ((-sparsityParam ./ avg_activation) + ((1-sparsityParam)./(1-avg_activation))),1, m);
%we use repmat here to create a matrix of sparisty delta to add to every element of (W2' * delta3)
delta2 = (W2' * delta3 + sparsity_delta) .* sigmoid_gradient(z1); %DIMEN: 2 x 10

W1grad = (delta2 * a1') ./ m + lambda * W1; %dimen: 2 x 64
W2grad = (delta3 * a2') ./ m + lambda * W2; %dimen: 64 x 2
b1grad = mean(delta2,2); 
b2grad = mean(delta3,2);

%converting gradients into vector so optimization algorithm can process it.

grad = [W1grad(:) ; W2grad(:) ; b1grad(:) ; b2grad(:)];

end

function sigm = sigmoid(x)
  
    sigm = 1 ./ (1 + exp(-x));
end


function sigm_grad = sigmoid_gradient(z)
  sigm_grad = sigmoid(z) .* ( 1 - sigmoid(z));
end
